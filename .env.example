# Copy to .env and update values as needed.

# --- Provider selection (CLI still takes precedence) ---
# PROVIDER=ollama
# PROVIDER=bedrock

# --- Common model selection (pass via --model in CLI) ---
# MODEL=llama3.1

# --- Ollama ---
# Example for local Ollama:
# OLLAMA_BASE_URL=http://localhost:11434
# Example for Ollama on local network:
# OLLAMA_BASE_URL=http://192.168.1.50:11434
OLLAMA_BASE_URL=http://localhost:11434

# --- Audio transcription (Ollama only) ---
# Whisper model size used for local speech-to-text transcription.
# Options: tiny, base, small, medium, large, large-v2, large-v3
# Larger models are more accurate but slower and require more RAM.
TRANSCRIPT_MODEL=base
# Ollama model used to convert raw transcript text into clean Markdown
MARKDOWN_MODEL=qwen3:8b

# --- AWS Bedrock ---
AWS_REGION=us-west-2
# Optional, if you use named AWS profile:
# AWS_PROFILE=default
# Optional static creds (prefer profile/role when possible):
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# AWS_SESSION_TOKEN=
# --- Suggested model examples ---
# Bedrock model ID example:
# MODEL=anthropic.claude-3-5-sonnet-20240620-v1:0

# --- Optional MCP weather service routing (FastMCP client) ---
# If MCP_SERVER_URL is set, weather tool calls are routed through MCP.
# MCP_SERVER_URL=http://localhost:8000/mcp
# MCP_TIMEOUT_SECONDS=20
# MCP_WEATHER_CURRENT_TOOL=current_weather
# MCP_WEATHER_FORECAST_TOOL=weather_forecast
